{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ff8a6b6a84b18fbf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Homework 3: Recurrent Neural Networks (100 points)\n",
    "\n",
    "### Overview\n",
    "\n",
    "We now move from image recognition to natural language processing. For this assignment, we will work with a common sentiment analysis task using the IMDB dataset. This set consists of review-label pairs, where the task is to predict whether the text of the given movie review is positive or negative, a binary classification.\n",
    "\n",
    "### RNN Architecture\n",
    "\n",
    "You will be comparing four different recurrent neural network architectures: a standard RNN, a Gated Recurrent Unit (GRU), a standard Long Short-Term Memory (LSTM), and a bidirectional LSTM. \n",
    "\n",
    "Note that a GRU/LSTM cell _is_ an RNN cell, but we will refer to an RNN in the code and questions below as the most basic RNN.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "At the bottom of this notebook file, there are three short answer questions testing your understanding of this neural network architecture. \n",
    "\n",
    "Below each question is a cell with the text “Type Markdown and LaTex.” Double-click the cell and type your response to the question. Save your responses by clicking on the floppy disk icon or choosing File - Save and Checkpoint.\n",
    "\n",
    "After responding to the questions, download your notebook as a `.html` file by choosing File - Download as - html (.html). You will be submitting this `.html` file to your instructor for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.set_num_threads(4)\n",
    "torch.set_num_interop_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'assets_week3'\n",
    "reviewVocabVectors = pickle.load(open(root_dir + '/reviewVocabVectors', 'rb'))\n",
    "trainIterator = pickle.load(open(root_dir + '/trainIterator', 'rb'))\n",
    "testIterator = pickle.load(open(root_dir + '/testIterator', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingSize = 100\n",
    "hiddenSize = 8\n",
    "dropoutRate = 0.5\n",
    "numEpochs = 2\n",
    "vocabSize = 20002\n",
    "pad = 1\n",
    "unk = 0\n",
    "\n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.name = model\n",
    "        self.LSTM = (model == 'LSTM' or model == 'BiLSTM')\n",
    "        self.bidir = (model == 'BiLSTM')\n",
    "        \n",
    "        self.embed = nn.Embedding(vocabSize, embeddingSize, padding_idx = pad)\n",
    "        \n",
    "        if model == 'RNN': \n",
    "            self.rnn = nn.RNN(embeddingSize, hiddenSize)\n",
    "        elif model == 'GRU': \n",
    "            self.rnn = nn.GRU(embeddingSize, hiddenSize)\n",
    "        else: \n",
    "            self.rnn = nn.LSTM(embeddingSize, hiddenSize, bidirectional=self.bidir)\n",
    "\n",
    "        self.dense = nn.Linear(hiddenSize * (2 if self.bidir else 1), 1)\n",
    "        self.dropout = nn.Dropout(dropoutRate)\n",
    "        \n",
    "    def forward(self, text, textLengths):\n",
    "        embedded = self.dropout(self.embed(text))\n",
    "        \n",
    "        packedEmbedded = nn.utils.rnn.pack_padded_sequence(embedded, textLengths)\n",
    "        if self.LSTM: \n",
    "            packedOutput, (hidden, cell) = self.rnn(packedEmbedded)\n",
    "        else: \n",
    "            packedOutput, hidden = self.rnn(packedEmbedded)\n",
    "\n",
    "        output, outputLengths = nn.utils.rnn.pad_packed_sequence(packedOutput)\n",
    "        if self.bidir: \n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        else: \n",
    "            hidden = hidden[0]\n",
    "\n",
    "        return self.dense(self.dropout(hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicRNN = MyRNN(model='RNN')\n",
    "GRU = MyRNN(model='GRU') # Construct a GRU model, as above\n",
    "LSTM = MyRNN(model='LSTM') # Construct a LSTM model, as above\n",
    "biLSTM = MyRNN(model='BiLSTM') # Construct a BiLSTM model, as above\n",
    "models = [basicRNN, GRU, LSTM, biLSTM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    if model is None:\n",
    "        continue\n",
    "    model.embed.weight.data.copy_(reviewVocabVectors)\n",
    "    model.embed.weight.data[unk] = torch.zeros(embeddingSize)\n",
    "    model.embed.weight.data[pad] = torch.zeros(embeddingSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def batchAccuracy(preds, targets):\n",
    "    roundedPreds = (preds >= 0)\n",
    "    return (roundedPreds == targets).sum().item() / len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RNN, Epoch: 1, Train Loss: 0.7126365525033468\n",
      "Model: RNN, Epoch: 2, Train Loss: 0.6936717201071931\n",
      "\n",
      "Model: GRU, Epoch: 1, Train Loss: 0.7025310509954877\n",
      "Model: GRU, Epoch: 2, Train Loss: 0.686497655366083\n",
      "\n",
      "Model: LSTM, Epoch: 1, Train Loss: 0.6937546568453464\n",
      "Model: LSTM, Epoch: 2, Train Loss: 0.6713911288839471\n",
      "\n",
      "Model: BiLSTM, Epoch: 1, Train Loss: 0.6949846438129844\n",
      "Model: BiLSTM, Epoch: 2, Train Loss: 0.6875493588959775\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "for model in models: \n",
    "    if model is not None:\n",
    "        model.train()\n",
    "\n",
    "for model in models:\n",
    "    if model is None:\n",
    "        continue\n",
    "        \n",
    "    torch.manual_seed(0)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    for epoch in range(numEpochs):\n",
    "        epochLoss = 0\n",
    "        for batch in trainIterator:\n",
    "            optimizer.zero_grad()\n",
    "            text, textLen = batch[0]\n",
    "            predictions = model(text, textLen).squeeze(1)\n",
    "            loss = criterion(predictions, batch[1])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epochLoss += loss.item()\n",
    "        print(f'Model: {model.name}, Epoch: {epoch + 1}, Train Loss: {epochLoss / len(trainIterator)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RNN, Validation Accuracy: 54.682704603580554%\n",
      "Model: GRU, Validation Accuracy: 63.08903452685421%\n",
      "Model: LSTM, Validation Accuracy: 74.12244245524298%\n",
      "Model: BiLSTM, Validation Accuracy: 58.70284526854219%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "for model in models: \n",
    "    if model is not None:\n",
    "        model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for model in models:\n",
    "        \n",
    "        if model is None:\n",
    "            continue\n",
    "\n",
    "        accuracy = 0.0\n",
    "        for batch in testIterator:\n",
    "            text, textLen = batch[0]\n",
    "            predictions = model(text, textLen).squeeze(1)\n",
    "            loss = criterion(predictions, batch[1])\n",
    "            acc = batchAccuracy(predictions, batch[1])\n",
    "            accuracy += acc\n",
    "        print('Model: {}, Validation Accuracy: {}%'.format(model.name, accuracy / len(testIterator) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6f36ec050380d95b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Homework Questions\n",
    "\n",
    "**To make sure your code produces consistent results, it is advisable to click \"Kernel -> Restart & Run All\" every time you want to run your code.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6a084a6888e27954",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 1: Coding (50 points)\n",
    "\n",
    "First, run the code given above to assess accuracy of the default RNN model. \n",
    "\n",
    "Next, you will need to construct three other model types (GRU, LSTM, BiLSTM) for comparison purposes. Follow the comments in box 5 to initialize the three other model types then rerun the code with all models enabled.\n",
    "\n",
    "Finally, compare the accuracies of all four models (the accuracy of the default RNN should not change from the initial run). Explain your results. In doing so, overview the advantages of the best performing architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-503b047d28162c58",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Validation Accuracy for RRN : ~70%\n",
    "\n",
    "Below are the validation accuracy in the 2nd run:\n",
    "\n",
    "Model: RNN, Validation Accuracy: 70.63459079283886%\n",
    "\n",
    "Model: GRU, Validation Accuracy: 82.07800511508951%\n",
    "\n",
    "Model: LSTM, Validation Accuracy: 83.81953324808184%\n",
    "\n",
    "Model: BiLSTM, Validation Accuracy: 80.93190537084399%\n",
    "\n",
    "Clearly LSTM is the best performing architecture for the imdb review classification.\n",
    "\n",
    "Here are some of the advantages of using LSTM for sentiment analysis:\n",
    "\n",
    "Ability to handle long-term dependencies: Since LSTM is specifically designed to address the vanishing gradient problem, it is able to effectively handle long-term dependencies in sequences. This is particularly useful for sentiment analysis, where the sentiment of a review may be influenced by words that are far apart in the sequence.\n",
    "\n",
    "Memory cells to remember important information: LSTMs use memory cells to remember important information from previous time steps, which helps the network to maintain context throughout the sequence. This is particularly useful for sentiment analysis, where the sentiment of a review may depend on the overall context of the review.\n",
    "\n",
    "Forget gates to filter out irrelevant information: LSTMs use forget gates to filter out irrelevant information from the previous time step, which helps the network to focus on the most important information for predicting the sentiment of the review. This is particularly useful for sentiment analysis, where there may be many words in a review that are not relevant to the overall sentiment.\n",
    "\n",
    "Although BiLSTM do not have the best accuracy for our case, but BiLSTM can process the sequence in both forward and backward directions, which helps the network to capture dependencies that may be missed by unidirectional models. This is particularly useful for sentiment analysis, where the sentiment of a review may depend on the overall context of the review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3920676c328b92ee",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 2: LSTM Gates (30 points)\n",
    "\n",
    "LSTMs improve upon the naive RNN architecture by adding a series of gates instead of a simple matrix-vector computation. Name the gates and explain each of their functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2bb845467aba5dd6",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "The 4 gates in LSTMs and their functions are:\n",
    "\n",
    "Input gate: This gate is responsible for deciding which new information to add to the cell state. It takes as input the previous hidden state and the current input and the output of the input gate layer is a vector of the same size as the cell state, with each element being a number between 0 and 1 that determines how much of the corresponding element in the new candidate vector to add to the cell state.\n",
    "\n",
    "Forget gate: This gate is responsible for deciding which information to discard from the cell state. It takes as input the previous hidden state and the current input and has a sigmoid function called the forget gate layer, which decides which values to forget. The output of the forget gate layer is a vector of the same size as the cell state, with each element being a number between 0 and 1 that determines how much of the corresponding element in the previous cell state to keep.\n",
    "\n",
    "Output gate: This gate is responsible for deciding which information from the cell state to output as the current hidden state. It takes as input the previous hidden state and the current input and has two components: a sigmoid function called the output gate layer, which decides which values to output, and a tanh function that scales the output values. The output of the output gate layer is a vector of the same size as the cell state, with each element being a number between 0 and 1 that determines how much of the corresponding element in the cell state to output.\n",
    "\n",
    "Update gate: The update gate is an additional gate that is used in some variants of LSTMs. It is responsible for deciding which new information to add to the cell state, similar to the input gate. However, it is implemented differently from the input gate in that it takes as input the previous hidden state, the current input, and the current cell state, and uses these three inputs to calculate a new candidate state. The update gate is then used to blend the new candidate state with the previous cell state.\n",
    "\n",
    "In summary, the input gate decides which new information to add to the cell state, the forget gate decides which information to discard from the cell state, the output gate decides which information to output as the current hidden state, and the update gate is used in some variants of LSTMs to update the cell state. Together, these gates allow LSTMs to selectively remember or forget information from previous time steps and to focus on the most relevant information for the current time step, making them well-suited for processing sequential data with long-term dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ab00430a80313063",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 3: Applications (20 points)\n",
    "\n",
    "LSTMs are used across many different fields, from music generation to sentiment classification to text generation. Where could they be useful in your life, whether at home, for your family, or in the workplace? Give a specific problem or application for an LSTM model that was not covered in the course slides (**though it can be related to the applications covered in the slides**). Then, with your application in mind, specifically identify the input to your model, the output from your model, and an applicable loss function. \n",
    "\n",
    "(As an optional extension, try implementing your LSTM on your own using the code framework given in this homework!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-51a908d0d4f3f225",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "I invest in stocks and would be great if I can make a model for price prediction.I could train an LSTM model to take in historical price and volume data for a given stock and predict its future prices.\n",
    "\n",
    "The input to my model would be a time series of historical price and volume data for the given stock, with each time step representing a single day or hour. The output would be a sequence of predicted future prices for the next n time steps.\n",
    "\n",
    "An applicable loss function for this problem could be mean squared error (MSE), which measures the average squared difference between the predicted and actual prices. This loss function is commonly used in regression problems and would help to penalize the model for large prediction errors.\n",
    "\n",
    "By using an LSTM model to predict stock prices, I could potentially make more informed decisions about buying and selling stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
