{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Feed-forward Neural Networks (100 points)\n",
    "\n",
    "### Overview\n",
    "\n",
    "Below you will find a PyTorch implementation of a feed-forward neural network for image recognition. We use the popular MNIST dataset, where the model predicts a single digit (0-9) for a black-and-white photo of a handwritten digit. This is a _classification_ task.\n",
    "\n",
    "### NN Architecture\n",
    "\n",
    "Each image has size 28x28 grayscale pixel values between 0 and 255. In preprocessing, we flatten each image to a single vector of length $28^2 = 784$, which serves as the entire input for the model.\n",
    "\n",
    "For each image, we aim to predict one of ten classes (0-9). We could use an output layer $y$ of size 1 (a single neuron) -- for example, using a naive mapping like prediction $p = \\mathrm{int}(10y)$. But this presupposes that a handwritten 0 is similar to a handwritten 1 and very different than a handwritten 9, which isn't the case. So instead we use an output layer $y$ of size 10, where the prediction $p = argmax(y)$, so each output neuron controls the likelihood for a particular class.\n",
    "\n",
    "We use a simple two-layer neural network. To begin, we will have an input size of 784, a hidden layer of size 5, and an output layer of size 10.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "At the bottom of this notebook file, there are a series of questions testing your understanding of this neural network architecture. Some questions include instructions where you will need to modify hyperparameters (notated in the code below) and re-run the model to investigate the changed results. __There is no need to read through the following code in depth to answer the questions, but it may be useful as a reference.__\n",
    "\n",
    "Below each question is a cell with the text “Type Markdown and LaTex.” Double-click the cell and type your response to the question. Save your responses by clicking on the floppy disk icon or choosing File - Save and Checkpoint.\n",
    "\n",
    "After responding to the questions, download your notebook as a `.html` file by choosing File - Download as - html (.html). You will be submitting this `.html` file to your instructor for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x772f2c216e70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'assets_week1'\n",
    "trainDataset = datasets.MNIST(root=root_dir, train=True, transform=transforms.ToTensor(), download=True)\n",
    "testDataset = datasets.MNIST(root=root_dir, train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModel(nn.Module):\n",
    "    def __init__(self, inputSize, outputSize, hiddenSize, activate):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.activate = nn.Sigmoid() if activate == \"Sigmoid\" else nn.Tanh() if activate == \"Tanh\" else nn.ReLU()\n",
    "        self.layer1 = nn.Linear(inputSize, hiddenSize)\n",
    "        self.layer2 = nn.Linear(hiddenSize, outputSize)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        hidden = self.activate(self.layer1(X))\n",
    "        return self.layer2(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dimensionality of the input\n",
    "inputSize = 784\n",
    "# Number of neurons in the first layer\n",
    "hiddenSize = 5 #300 #5\n",
    "# Number of neurons in the second layer\n",
    "outputSize = 10\n",
    "# Activation function (default: ReLU, options: Sigmoid, Tanh, ReLU)\n",
    "activation = \"Relu\" #\"Sigmoid\"\n",
    "# Learning rate\n",
    "learningRate = .001 #1 #0.001\n",
    "# Number of training epochs\n",
    "numEpochs = 5 #10 #5\n",
    "# Number of training examples per batch\n",
    "batchSize = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Beginning training!\n",
      "Epoch [1/5], Step [100/300], Loss: 1.5429329872131348\n",
      "Epoch [1/5], Step [200/300], Loss: 1.1821335554122925\n",
      "Epoch [1/5], Step [300/300], Loss: 1.0662295818328857\n",
      "Epoch [2/5], Step [100/300], Loss: 0.8010011315345764\n",
      "Epoch [2/5], Step [200/300], Loss: 0.6519669890403748\n",
      "Epoch [2/5], Step [300/300], Loss: 0.5987505912780762\n",
      "Epoch [3/5], Step [100/300], Loss: 0.5415259003639221\n",
      "Epoch [3/5], Step [200/300], Loss: 0.5746868252754211\n",
      "Epoch [3/5], Step [300/300], Loss: 0.5670210719108582\n",
      "Epoch [4/5], Step [100/300], Loss: 0.39370736479759216\n",
      "Epoch [4/5], Step [200/300], Loss: 0.5235522985458374\n",
      "Epoch [4/5], Step [300/300], Loss: 0.5720871090888977\n",
      "Epoch [5/5], Step [100/300], Loss: 0.5311781764030457\n",
      "Epoch [5/5], Step [200/300], Loss: 0.737825334072113\n",
      "Epoch [5/5], Step [300/300], Loss: 0.5962098836898804\n",
      "\n",
      ">>> Beginning validation!\n",
      "Validation accuracy: 87.69%\n"
     ]
    }
   ],
   "source": [
    "trainLoader = torch.utils.data.DataLoader(dataset=trainDataset, batch_size=batchSize, shuffle=True)\n",
    "testLoader = torch.utils.data.DataLoader(dataset=testDataset, batch_size=batchSize, shuffle=False)\n",
    "\n",
    "net = NNModel(inputSize, outputSize, hiddenSize, activation)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learningRate)\n",
    "\n",
    "print('>>> Beginning training!')\n",
    "for epoch in range(numEpochs):\n",
    "    for i, (images, labels) in enumerate(trainLoader):\n",
    "        images = images.view(-1, 28*28)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagation\n",
    "        outputs = net(images)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient descent\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Logging\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {}'.format(epoch+1, numEpochs, i+1, len(trainDataset)//batchSize, loss))\n",
    "\n",
    "print()\n",
    "print('>>> Beginning validation!')\n",
    "correct, total = 0, 0\n",
    "for i, (images, labels) in enumerate(testLoader):\n",
    "    images = images.view(-1, 28*28)\n",
    "    \n",
    "    outputs = net(images)\n",
    "    _, prediction = torch.max(outputs, axis=1)\n",
    "    correct += torch.sum(prediction == labels)\n",
    "    total += labels.size(0)\n",
    "print('Validation accuracy: {}%'.format(correct.item()/total*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5e66e36a749e1d3e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Homework Questions\n",
    "\n",
    "Your goal is to improve the model's accuracy by tuning hyperparameters. If a question asks you to modify a hyperparameter and you obtain improved results, retain that hyperparameter change for subsequent questions. Otherwise, revert back to the original hyperparameter value.\n",
    "\n",
    "**To make sure your code produces consistent results, it is advisable to click \"Kernel -> Restart & Run All\" every time you want to run your code.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f7531bbce136967d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 1: Loss Minimization & Gradient Descent (5 points)\n",
    "\n",
    "Given a neural network with model parameters $\\theta$, loss function $E$, and learning rate $\\alpha$, what is the correct method to perform gradient descent?\n",
    "\n",
    "a) $\\theta_i += \\alpha E$\n",
    "\n",
    "b) $\\theta_i -= \\alpha E$\n",
    "\n",
    "c) $\\theta_i += \\alpha\\frac{\\partial E}{\\partial \\theta_i}$\n",
    "\n",
    "d) $\\theta_i -= \\alpha\\frac{\\partial E}{\\partial \\theta_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a78c77dc1b23ccb9",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "d is the correct method to perform gradient descent. We take the slope of the curve and move in the direction with a learning rate of alpha till we reach minima or till the gradient becomes very small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7257d9aaf2ae5f52",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 2: Class Imbalance (10 points)\n",
    "\n",
    "Imagine you are an engineer tasked with helping a company to identify faulty parts early using an machine learning-based image recognition system. What evaluation metric would you use? More specifically, explain why a raw percent accuracy score would be a poor choice of evaluation metric for this problem space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d1ceb13ddbf02d25",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Class imbalance would mean that there are significantly more instances of one class (non-faulty parts) than the other (faulty parts).\n",
    "\n",
    "Simple accuracy as the evaluation metric would lead to an over-optimistic estimate of the model's performance, as it could achieve a high accuracy score by always predicting the majority class (non-faulty parts) without detecting any faulty parts.\n",
    "\n",
    "Therefore other evaluation metrics such as precision and recall would be more suitable. Precision measures the proportion of predicted faulty parts that are correctly classified as faulty. Recall measures the proportion of actual faulty parts that are correctly identified as faulty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7e740ab84baa5c87",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 3a:  Size of a Hidden Layer (10 points)\n",
    "\n",
    "Explain how the hidden layer size influences the architecture of a feed-forward neural network. In doing so, note what can happen if the hidden size is too large and what can happen if the hidden size is too small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0756abe8399f8d1b",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "If the hidden layer size is too small, the neural network may not be able to learn complex patterns in the data resulting in poor performance on both the training and testing data.\n",
    "\n",
    "If the hidden layer size is too large, the neural network may become too complex and may overfit the training data. This means that the model may perform well on the training data but poorly on new data.\n",
    "\n",
    "Therefore we need to do some hyperparameter tuning to get the best number of hidden layers for the dataset we are interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8918fa8441b6410a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 3b: Size of a Hidden Layer  (10 points)\n",
    "\n",
    "Increase the hidden size from 5 to 300 and re-run your trial. How does the accuracy change?\n",
    "\n",
    "_a) It increases, since the model learns more quickly_\n",
    "\n",
    "_b) It increases, since the model has more memory and can learn more complex features_\n",
    "\n",
    "_c) It decreases, since the model has to learn more parameters and it doesn't have enough time_\n",
    "\n",
    "_d) It decreases, since the model has less memory_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-fa1da64d2d3807df",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Accuracy w.r.t 5 hidden layer:\n",
    "Validation accuracy: 74.42999999999999%\n",
    "\n",
    "Accuracy w.r.t 300 hidden layer:\n",
    "Validation accuracy: 95.48%\n",
    "\n",
    "The accuracy seems to have jumped when we increased the hidden layers from 5 to 300 but the training took some more time, may be because model had to learn additional number of parameters.\n",
    "\n",
    "There correct answer is b - accuracy increases, since the model has more memory and can learn more complex features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-df1fbf77fef73231",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 4a: Learning Rate  (10 points)\n",
    "\n",
    "Explain the purpose of a learning rate. In doing so, note what can happen if the learning rate is too large and what can happen if the learning rate is too small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-01781cd504996df3",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "The learning rate controls the step size at which the model's parameters are updated during training. In other words, the learning rate determines how quickly or slowly the model learns from the data.If the learning rate is too large, the model may overshoot the optimal solution.\n",
    "\n",
    "If the learning rate is too small, the model may take too long to converge and may get stuck in local optima ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e14ac17232cdd618",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 4b: Learning Rate  (10 points)\n",
    "\n",
    "Increase the learning rate from 0.001 to 1. How does the accuracy change?\n",
    "\n",
    "a) It increases, since the model learns more quickly\n",
    "\n",
    "b) It increases, since the model is better able to converge\n",
    "\n",
    "c) It decreases, since the model learns too slowly\n",
    "\n",
    "d) It decreases, since the model is not able to converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-7fd150248c4bd476",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Accuracy w.r.t learning rate of 0.001:\n",
    "Validation accuracy: 75.64999999999999%\n",
    "\n",
    "Accuracy w.r.t learning rate of 1:\n",
    "Validation accuracy: 58.37%\n",
    "\n",
    "To know if accuracy decreases because the model learned too slowly or model is not able to converge, lets increase the number of epochs to 10 and re run the model.\n",
    "\n",
    "Accuracy w.r.t learning rate of 1 and numEpochs =10 (not 5):\n",
    "Validation accuracy: 19.470000000000002%\n",
    "\n",
    "Increasing the number of epochs further decreases the accuracy meaning that model is actually not able to converge, hence answer d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d7c6dda3a2679bae",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 5a: Activation Functions (10 points)\n",
    "\n",
    "Explain the main purpose of an activation function in neural networks. Also, explain the main benefit of the Tanh activation function over the Sigmoid activation function, and the main benefit of the ReLU activation function over the Sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-581edc895a350827",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "The main purpose of an activation function is to introduce non-linearity to the output of a neuron. Non-linearity means that the output is not just a linear combination of the inputs, but a more complex function that can better capture the relationships between the inputs and the outputs.\n",
    "\n",
    "The Tanh activation function is similar to the Sigmoid activation function, but it outputs values between -1 and 1 instead of 0 and 1.The main benefit of using the Tanh function over the Sigmoid function is that it can help prevent the vanishing gradient problem, which can occur when the gradient of the Sigmoid function becomes very small for very large or very small inputs.\n",
    "\n",
    "The ReLU (Rectified Linear Unit) activation function is a very simple function that outputs the input value if it is positive, and 0 otherwise. This means that it introduces non-linearity to the output of a neuron in a very simple way. The main benefit of using the ReLU function over the Sigmoid function is that it is computationally efficient and can help prevent the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d49d5273a50005b5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 5b: Activation Functions (5 points)\n",
    "\n",
    "Change the activation function in the hyperparameter list above to determine which activation function is most effective at this task.\n",
    "\n",
    "a) ReLU\n",
    "\n",
    "b) Sigmoid\n",
    "\n",
    "c) Tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2c7a175daa0a705e",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Accuracy with Sigmoid:\n",
    "Validation accuracy: 78.73%\n",
    "\n",
    "Accuracy with Tanh:\n",
    "Validation accuracy: 83.3%\n",
    "\n",
    "Accuracy with Relu:\n",
    "Validation accuracy: 85.41%\n",
    "\n",
    "so Relu is the most effective activation function for this task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-aa7221b14bf17a26",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 6: Overfitting  (10 points)\n",
    "\n",
    "Define overfitting and explain how it can damage model training and results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-21faea75167bd4eb",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Overfitting is a problem in machine learning, which occurs when a model learns the training data too well, to the extent that it captures the noise and randomness in the data, rather than the underlying patterns. This can lead to poor performance when the model is applied to new, unseen data.\n",
    "\n",
    "The problem with overfitting is that the model becomes too sensitive to the noise and randomness in the training data and loses its ability to generalize to new data. This means that the model may perform well on the training data, but perform poorly on the test data or real-world data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-75a57b7d602e6ffc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 7: Early Stopping  (10 points)\n",
    "\n",
    "Outline a procedure for early stopping to prevent overfitting. Clearly describe how you’d use the training, validation, and test sets accuracy to decide when to stop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-40b7e3a1cce86696",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Early stopping is a technique used in machine learning to prevent overfitting by stopping the training process when the model's performance on the validation set stops improving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5ea01f05290cc74e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 8: Regularization  (10 points)\n",
    "\n",
    "Briefly explain a few common methods of regularization to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e4fbdc9819855660",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "L1 Regularization (Lasso Regression): This method adds a penalty term to the loss function, proportional to the absolute values of the model parameters (also known as L1 norm). This encourages the model to have sparse weight vectors, meaning some weights are forced to zero. This makes the model simpler and less prone to overfitting.\n",
    "\n",
    "L2 Regularization (Ridge Regression): This method adds a penalty term to the loss function, proportional to the square of the model parameters (also known as L2 norm). This encourages the model to have small weight vectors, reducing the impact of large weights on the output, making the model more generalizable.\n",
    "\n",
    "Dropout Regularization: This method randomly drops out a percentage of the neurons in the network during training, forcing the model to learn more robust features, reducing its sensitivity to individual weights, and thus overfitting. \n",
    "\n",
    "Early Stopping: This method stops the training process when the model's performance on the validation set stops improving, preventing the model from overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
