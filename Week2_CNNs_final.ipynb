{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6a65256d7eef6e7d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Homework 2: Convolutional Neural Networks (100 points)\n",
    "\n",
    "### Overview\n",
    "\n",
    "With new knowledge of convolutional neural networks, we can accomplish a more difficult image recognition task. The CIFAR-10 classification dataset consists of 60,000 labelled images split between 10 classes: airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships and trucks.\n",
    "\n",
    "For the purposes of this assignment, we will compare two models on the same dataset: a fully connected neural network (as in Homework 1) called ANN and a new convolutional architecture called CNN, as outlined in the next section. To be fair, we attempt to allow the same number of trainable parameters in the ANN as the CNN, which means we need to use the same input transformation to flatten grayscale used in Homework 1 for the ANN. The CNN reaps the full benefit of the original 2D image in RGB.\n",
    "\n",
    "### CNN Architecture\n",
    "\n",
    "Each image consists of 32x32 RGB pixel values between 0 and 255. We do not need to perform any preprocessing as the convolutional model will use all three channels concurrently as input.\n",
    "\n",
    "The architecture in use has 5 layers: a convolution layer followed by a pooling layer, then another convolutional layer, then two fully connected dense layers. The latter of these has 10 neurons to provide classification output.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "At the bottom of this notebook file, there are four short answer questions testing your understanding of this neural network architecture. As before, some questions will require you to experiment with model hyperparameters.\n",
    "\n",
    "Below each question is a cell with the text “Type Markdown and LaTex.” Double-click the cell and type your response to the question. Save your responses by clicking on the floppy disk icon or choosing File - Save and Checkpoint.\n",
    "\n",
    "After responding to the questions, download your notebook as a `.html` file by choosing File - Download as - html (.html). You will be submitting this `.html` file to your instructor for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.set_num_threads(4)\n",
    "torch.set_num_interop_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original transform\n",
    "# trainTransform = transforms.Compose([#add yours here!\n",
    "#                                      transforms.ToTensor(),\n",
    "#                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "#                                     ])\n",
    "# testTransform = transforms.Compose([transforms.ToTensor(),\n",
    "#                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "#                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTransform = transforms.Compose([transforms.RandomRotation(5),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                    ])\n",
    "testTransform = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "root_dir = 'assets_week2'\n",
    "trainDataset = torchvision.datasets.CIFAR10(root=root_dir, train=True, download=True, transform=trainTransform)\n",
    "trainLoader = torch.utils.data.DataLoader(trainDataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "testDataset = torchvision.datasets.CIFAR10(root=root_dir, train=False, download=True, transform=testTransform)\n",
    "testLoader = torch.utils.data.DataLoader(testDataset, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel(nn.Module):\n",
    "    def __init__(self, hiddenSize, dropoutRate, activate):\n",
    "        super().__init__()\n",
    "        # Note that 'layer' and 'dense' differ only in name (to show similarity to CNN)\n",
    "        self.activate = nn.Sigmoid() if activate == \"Sigmoid\" else nn.ReLU()\n",
    "        self.layer1 = nn.Linear(1024, 100)\n",
    "        self.layer2 = nn.Linear(100, 15 * 5 * 5)\n",
    "        self.dense1 = nn.Linear(15 * 5 * 5, hiddenSize)\n",
    "        self.dropout = nn.Dropout(dropoutRate)\n",
    "        self.dense2 = nn.Linear(hiddenSize, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activate(self.layer1(x))\n",
    "        x = self.activate(self.layer2(x))\n",
    "        x = self.dropout(self.activate(self.dense1(x)))\n",
    "        return self.dense2(x)\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, hiddenSize, outChannels, dropoutRate, activate):\n",
    "        super().__init__()\n",
    "        self.outChannels = outChannels\n",
    "        self.activate = nn.Sigmoid() if activate == \"Sigmoid\" else nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(3, 24, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(24, outChannels, 5)\n",
    "        self.dense1 = nn.Linear(outChannels * 5 * 5, hiddenSize)\n",
    "        self.dropout = nn.Dropout(dropoutRate)\n",
    "        self.dense2 = nn.Linear(hiddenSize, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.activate(self.conv1(x)))\n",
    "        x = self.pool(self.activate(self.conv2(x)))\n",
    "        x = x.view(-1, self.outChannels * 5 * 5)\n",
    "        x = self.dropout(self.activate(self.dense1(x)))\n",
    "        return self.dense2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of neurons in the first fully-connected layer\n",
    "hiddenSize = 100\n",
    "# Number of feature filters in second convolutional layer\n",
    "numFilters = 25\n",
    "# Dropout rate\n",
    "dropoutRate = 0\n",
    "# Activation function\n",
    "activation = \"ReLU\"\n",
    "# Learning rate\n",
    "learningRate = 0.001\n",
    "# Momentum for SGD optimizer\n",
    "momentum = 0.9\n",
    "# Number of training epochs\n",
    "numEpochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Beginning training!\n",
      "Epoch [1/10], Step [2000/12500], ANN Loss: 2.0651431404650213, CNN Loss: 2.011094207406044\n",
      "Epoch [1/10], Step [4000/12500], ANN Loss: 1.9222866840958595, CNN Loss: 1.651698141783476\n",
      "Epoch [1/10], Step [6000/12500], ANN Loss: 1.866415710836649, CNN Loss: 1.501900658980012\n",
      "Epoch [1/10], Step [8000/12500], ANN Loss: 1.8486922891438007, CNN Loss: 1.4304361519664526\n",
      "Epoch [1/10], Step [10000/12500], ANN Loss: 1.819504773169756, CNN Loss: 1.3662658289670944\n",
      "Epoch [1/10], Step [12000/12500], ANN Loss: 1.792697628468275, CNN Loss: 1.294509585440159\n",
      "Epoch [2/10], Step [2000/12500], ANN Loss: 1.7398476891368628, CNN Loss: 1.2263958215303719\n",
      "Epoch [2/10], Step [4000/12500], ANN Loss: 1.721903137549758, CNN Loss: 1.2103745007775724\n",
      "Epoch [2/10], Step [6000/12500], ANN Loss: 1.7148346040248872, CNN Loss: 1.1875471148602665\n",
      "Epoch [2/10], Step [8000/12500], ANN Loss: 1.7160489804148673, CNN Loss: 1.1464335404336452\n",
      "Epoch [2/10], Step [10000/12500], ANN Loss: 1.7132752705663443, CNN Loss: 1.1285078466879204\n",
      "Epoch [2/10], Step [12000/12500], ANN Loss: 1.7002093015909194, CNN Loss: 1.1023444667328148\n",
      "Epoch [3/10], Step [2000/12500], ANN Loss: 1.6477987474501132, CNN Loss: 1.0322250591833144\n",
      "Epoch [3/10], Step [4000/12500], ANN Loss: 1.6337226323038339, CNN Loss: 1.0408614556547255\n",
      "Epoch [3/10], Step [6000/12500], ANN Loss: 1.6474793972000479, CNN Loss: 1.0584902813639492\n",
      "Epoch [3/10], Step [8000/12500], ANN Loss: 1.6477712057083844, CNN Loss: 1.0075805018562825\n",
      "Epoch [3/10], Step [10000/12500], ANN Loss: 1.6051863313913346, CNN Loss: 0.9981298083662987\n",
      "Epoch [3/10], Step [12000/12500], ANN Loss: 1.6276315301209687, CNN Loss: 1.016325661229901\n",
      "Epoch [4/10], Step [2000/12500], ANN Loss: 1.5762041495591401, CNN Loss: 0.932757071968168\n",
      "Epoch [4/10], Step [4000/12500], ANN Loss: 1.5712711747586727, CNN Loss: 0.9292551703359931\n",
      "Epoch [4/10], Step [6000/12500], ANN Loss: 1.5976572595238685, CNN Loss: 0.9456884891419323\n",
      "Epoch [4/10], Step [8000/12500], ANN Loss: 1.5901475484520198, CNN Loss: 0.9393124193107942\n",
      "Epoch [4/10], Step [10000/12500], ANN Loss: 1.5551417142748833, CNN Loss: 0.9434110709745437\n",
      "Epoch [4/10], Step [12000/12500], ANN Loss: 1.5827486747652293, CNN Loss: 0.9280422668519314\n",
      "Epoch [5/10], Step [2000/12500], ANN Loss: 1.5165686519816517, CNN Loss: 0.8437924679678399\n",
      "Epoch [5/10], Step [4000/12500], ANN Loss: 1.5485205021798611, CNN Loss: 0.8810495305859949\n",
      "Epoch [5/10], Step [6000/12500], ANN Loss: 1.5205376298725606, CNN Loss: 0.875854483172996\n",
      "Epoch [5/10], Step [8000/12500], ANN Loss: 1.5323724678643047, CNN Loss: 0.8874376007250103\n",
      "Epoch [5/10], Step [10000/12500], ANN Loss: 1.5314961768835782, CNN Loss: 0.881722926983377\n",
      "Epoch [5/10], Step [12000/12500], ANN Loss: 1.536129963710904, CNN Loss: 0.8707152424072264\n",
      "Epoch [6/10], Step [2000/12500], ANN Loss: 1.4802862400189043, CNN Loss: 0.8027446420188062\n",
      "Epoch [6/10], Step [4000/12500], ANN Loss: 1.476063256882131, CNN Loss: 0.8031092129009776\n",
      "Epoch [6/10], Step [6000/12500], ANN Loss: 1.4967780131846666, CNN Loss: 0.8226775566637516\n",
      "Epoch [6/10], Step [8000/12500], ANN Loss: 1.5043121395856143, CNN Loss: 0.8403089594286867\n",
      "Epoch [6/10], Step [10000/12500], ANN Loss: 1.4949796938300133, CNN Loss: 0.8336745113246142\n",
      "Epoch [6/10], Step [12000/12500], ANN Loss: 1.4943062402904033, CNN Loss: 0.833450839959085\n",
      "Epoch [7/10], Step [2000/12500], ANN Loss: 1.4377242470234632, CNN Loss: 0.7488008149513043\n",
      "Epoch [7/10], Step [4000/12500], ANN Loss: 1.4312388415560127, CNN Loss: 0.7795493462224258\n",
      "Epoch [7/10], Step [6000/12500], ANN Loss: 1.4408025848194956, CNN Loss: 0.7765507842549123\n",
      "Epoch [7/10], Step [8000/12500], ANN Loss: 1.4820247048214077, CNN Loss: 0.8059176552749705\n",
      "Epoch [7/10], Step [10000/12500], ANN Loss: 1.4745022121742368, CNN Loss: 0.8043968088396359\n",
      "Epoch [7/10], Step [12000/12500], ANN Loss: 1.476308487988077, CNN Loss: 0.803232390450663\n",
      "Epoch [8/10], Step [2000/12500], ANN Loss: 1.3793731155879796, CNN Loss: 0.6915573384351883\n",
      "Epoch [8/10], Step [4000/12500], ANN Loss: 1.4132366844788193, CNN Loss: 0.739756063252571\n",
      "Epoch [8/10], Step [6000/12500], ANN Loss: 1.4262815335281194, CNN Loss: 0.7608903559888713\n",
      "Epoch [8/10], Step [8000/12500], ANN Loss: 1.443523860409856, CNN Loss: 0.7480962942898622\n",
      "Epoch [8/10], Step [10000/12500], ANN Loss: 1.4404414008408786, CNN Loss: 0.752910276858136\n",
      "Epoch [8/10], Step [12000/12500], ANN Loss: 1.445216628395021, CNN Loss: 0.7832948234134819\n",
      "Epoch [9/10], Step [2000/12500], ANN Loss: 1.3494324534758926, CNN Loss: 0.6762504211796914\n",
      "Epoch [9/10], Step [4000/12500], ANN Loss: 1.381182004891336, CNN Loss: 0.6967163409851491\n",
      "Epoch [9/10], Step [6000/12500], ANN Loss: 1.4121976839527488, CNN Loss: 0.7333872945305193\n",
      "Epoch [9/10], Step [8000/12500], ANN Loss: 1.4008915499709547, CNN Loss: 0.7261405327457469\n",
      "Epoch [9/10], Step [10000/12500], ANN Loss: 1.4091352163776756, CNN Loss: 0.7291053080252022\n",
      "Epoch [9/10], Step [12000/12500], ANN Loss: 1.4186538611128927, CNN Loss: 0.7544252856955863\n",
      "Epoch [10/10], Step [2000/12500], ANN Loss: 1.348081712121144, CNN Loss: 0.6401631827008386\n",
      "Epoch [10/10], Step [4000/12500], ANN Loss: 1.336207102663815, CNN Loss: 0.6826209650248056\n",
      "Epoch [10/10], Step [6000/12500], ANN Loss: 1.3368962955512107, CNN Loss: 0.6803972842661024\n",
      "Epoch [10/10], Step [8000/12500], ANN Loss: 1.4035576878599823, CNN Loss: 0.7185519692246235\n",
      "Epoch [10/10], Step [10000/12500], ANN Loss: 1.3830730506032705, CNN Loss: 0.7250817751423456\n",
      "Epoch [10/10], Step [12000/12500], ANN Loss: 1.3811912578102201, CNN Loss: 0.7081062070974149\n",
      "\n",
      ">>> Beginning validation!\n",
      "ANN validation accuracy: 41.68%, CNN validation accuracy: 69.1%\n"
     ]
    }
   ],
   "source": [
    "ann = ANNModel(hiddenSize, dropoutRate, activation)\n",
    "cnn = CNNModel(hiddenSize, numFilters, dropoutRate, activation)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(list(ann.parameters()) + list(cnn.parameters()), lr=learningRate, momentum=momentum)\n",
    "\n",
    "print('>>> Beginning training!') \n",
    "ann.train()\n",
    "cnn.train()\n",
    "for epoch in range(numEpochs):  # loop over the dataset multiple times\n",
    "    annRunningLoss, cnnRunningLoss = 0, 0\n",
    "    for i, (inputs, labels) in enumerate(trainLoader, 0):\n",
    "        annInputs = torch.sum(inputs, axis=1).view(-1, 32*32)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward propagation\n",
    "        annOutputs = ann(annInputs)\n",
    "        cnnOutputs = cnn(inputs)\n",
    "        \n",
    "        # Backpropagation\n",
    "        annLoss = criterion(annOutputs, labels)\n",
    "        cnnLoss = criterion(cnnOutputs, labels)\n",
    "        annLoss.backward()\n",
    "        cnnLoss.backward()\n",
    "        \n",
    "        # Gradient update\n",
    "        optimizer.step()\n",
    "\n",
    "        annRunningLoss += annLoss.item()\n",
    "        cnnRunningLoss += cnnLoss.item()\n",
    "        if (i+1) % 2000 == 0:    # print every 2000 mini-batches\n",
    "            print('Epoch [{}/{}], Step [{}/{}], ANN Loss: {}, CNN Loss: {}'.format(epoch + 1, numEpochs, i + 1, len(trainDataset)//4, annRunningLoss/2000, cnnRunningLoss/2000))\n",
    "            annRunningLoss, cnnRunningLoss = 0, 0\n",
    "\n",
    "print()\n",
    "print('>>> Beginning validation!')\n",
    "ann.eval()\n",
    "cnn.eval()\n",
    "annCorrect, cnnCorrect = 0, 0\n",
    "total = 0\n",
    "for inputs, labels in testLoader:\n",
    "    annInputs = torch.sum(inputs, axis=1).view(-1, 32*32)\n",
    "    annOutputs = ann(annInputs)\n",
    "    cnnOutputs = cnn(inputs)\n",
    "    _, annPredicted = torch.max(annOutputs.data, 1)\n",
    "    _, cnnPredicted = torch.max(cnnOutputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    annCorrect += (annPredicted == labels).sum().item()\n",
    "    cnnCorrect += (cnnPredicted == labels).sum().item()\n",
    "print('ANN validation accuracy: {}%, CNN validation accuracy: {}%'.format(annCorrect / total * 100, cnnCorrect / total * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original performance result\n",
    "# >>> Beginning training!\n",
    "# Epoch [1/10], Step [2000/12500], ANN Loss: 2.062768382251263, CNN Loss: 2.0118962318003177\n",
    "# Epoch [1/10], Step [4000/12500], ANN Loss: 1.922449570864439, CNN Loss: 1.6491626717150212\n",
    "# Epoch [1/10], Step [6000/12500], ANN Loss: 1.8609392429888247, CNN Loss: 1.496796541929245\n",
    "# Epoch [1/10], Step [8000/12500], ANN Loss: 1.8343419369757175, CNN Loss: 1.4247962787225843\n",
    "# Epoch [1/10], Step [10000/12500], ANN Loss: 1.8108680999577045, CNN Loss: 1.3564418687969446\n",
    "# Epoch [1/10], Step [12000/12500], ANN Loss: 1.7755021077692508, CNN Loss: 1.2840593818947672\n",
    "# Epoch [2/10], Step [2000/12500], ANN Loss: 1.7118807710856199, CNN Loss: 1.217776041738689\n",
    "# Epoch [2/10], Step [4000/12500], ANN Loss: 1.7023028466850518, CNN Loss: 1.1937822642866522\n",
    "# Epoch [2/10], Step [6000/12500], ANN Loss: 1.6855913656800985, CNN Loss: 1.169634734245017\n",
    "# Epoch [2/10], Step [8000/12500], ANN Loss: 1.6861593794375658, CNN Loss: 1.1278142586536706\n",
    "# Epoch [2/10], Step [10000/12500], ANN Loss: 1.681460771769285, CNN Loss: 1.11159008590132\n",
    "# Epoch [2/10], Step [12000/12500], ANN Loss: 1.6844521416574716, CNN Loss: 1.093757688254118\n",
    "# Epoch [3/10], Step [2000/12500], ANN Loss: 1.5980612008571624, CNN Loss: 1.0147096219323575\n",
    "# Epoch [3/10], Step [4000/12500], ANN Loss: 1.5933891667425633, CNN Loss: 1.0154092156793921\n",
    "# Epoch [3/10], Step [6000/12500], ANN Loss: 1.6101581390574575, CNN Loss: 1.0260999397682027\n",
    "# Epoch [3/10], Step [8000/12500], ANN Loss: 1.623055516809225, CNN Loss: 0.9893379398109391\n",
    "# Epoch [3/10], Step [10000/12500], ANN Loss: 1.5694836803525687, CNN Loss: 0.9597428971934132\n",
    "# Epoch [3/10], Step [12000/12500], ANN Loss: 1.602022365361452, CNN Loss: 0.986960273547098\n",
    "# Epoch [4/10], Step [2000/12500], ANN Loss: 1.5354256336390972, CNN Loss: 0.8970438048327342\n",
    "# Epoch [4/10], Step [4000/12500], ANN Loss: 1.5316589372605085, CNN Loss: 0.9051831461470574\n",
    "# Epoch [4/10], Step [6000/12500], ANN Loss: 1.5507742081359028, CNN Loss: 0.9081797932619229\n",
    "# Epoch [4/10], Step [8000/12500], ANN Loss: 1.5432515789866448, CNN Loss: 0.8950935913156718\n",
    "# Epoch [4/10], Step [10000/12500], ANN Loss: 1.5296328798532486, CNN Loss: 0.9136955788973719\n",
    "# Epoch [4/10], Step [12000/12500], ANN Loss: 1.5434830002486706, CNN Loss: 0.8828933099175338\n",
    "# Epoch [5/10], Step [2000/12500], ANN Loss: 1.452810761027038, CNN Loss: 0.794904001011746\n",
    "# Epoch [5/10], Step [4000/12500], ANN Loss: 1.4939219054579735, CNN Loss: 0.8356246943548322\n",
    "# Epoch [5/10], Step [6000/12500], ANN Loss: 1.4754717657081782, CNN Loss: 0.842789939025417\n",
    "# Epoch [5/10], Step [8000/12500], ANN Loss: 1.48746871900931, CNN Loss: 0.8491292101730942\n",
    "# Epoch [5/10], Step [10000/12500], ANN Loss: 1.4935839128568769, CNN Loss: 0.8415487864455208\n",
    "# Epoch [5/10], Step [12000/12500], ANN Loss: 1.489127569541335, CNN Loss: 0.8365766553091817\n",
    "# Epoch [6/10], Step [2000/12500], ANN Loss: 1.4074806213155389, CNN Loss: 0.7510692907674238\n",
    "# Epoch [6/10], Step [4000/12500], ANN Loss: 1.4143287734538317, CNN Loss: 0.7461342083658091\n",
    "# Epoch [6/10], Step [6000/12500], ANN Loss: 1.4441396532729267, CNN Loss: 0.7792961364542134\n",
    "# Epoch [6/10], Step [8000/12500], ANN Loss: 1.4471339211985468, CNN Loss: 0.8049173939772881\n",
    "# Epoch [6/10], Step [10000/12500], ANN Loss: 1.436086897470057, CNN Loss: 0.789288026013528\n",
    "# Epoch [6/10], Step [12000/12500], ANN Loss: 1.450567073944956, CNN Loss: 0.7796823141543427\n",
    "# Epoch [7/10], Step [2000/12500], ANN Loss: 1.3581053456217052, CNN Loss: 0.6901736634639092\n",
    "# Epoch [7/10], Step [4000/12500], ANN Loss: 1.368674589846283, CNN Loss: 0.7057648525964468\n",
    "# Epoch [7/10], Step [6000/12500], ANN Loss: 1.369694784026593, CNN Loss: 0.7207892820711713\n",
    "# Epoch [7/10], Step [8000/12500], ANN Loss: 1.4126161734834313, CNN Loss: 0.765363468083553\n",
    "# Epoch [7/10], Step [10000/12500], ANN Loss: 1.4122630998045207, CNN Loss: 0.7621235234646593\n",
    "# Epoch [7/10], Step [12000/12500], ANN Loss: 1.409927348356694, CNN Loss: 0.7461324564729875\n",
    "# Epoch [8/10], Step [2000/12500], ANN Loss: 1.2936645967960358, CNN Loss: 0.6263135505313954\n",
    "# Epoch [8/10], Step [4000/12500], ANN Loss: 1.320595586769283, CNN Loss: 0.6764677218067227\n",
    "# Epoch [8/10], Step [6000/12500], ANN Loss: 1.346045205898583, CNN Loss: 0.6883500699620927\n",
    "# Epoch [8/10], Step [8000/12500], ANN Loss: 1.3628277169056238, CNN Loss: 0.700319167233436\n",
    "# Epoch [8/10], Step [10000/12500], ANN Loss: 1.366694432295859, CNN Loss: 0.7077743821138284\n",
    "# Epoch [8/10], Step [12000/12500], ANN Loss: 1.3888688515126706, CNN Loss: 0.7156861224401364\n",
    "# Epoch [9/10], Step [2000/12500], ANN Loss: 1.2629833322651685, CNN Loss: 0.5982729022389977\n",
    "# Epoch [9/10], Step [4000/12500], ANN Loss: 1.3036618775427342, CNN Loss: 0.6152381861356844\n",
    "# Epoch [9/10], Step [6000/12500], ANN Loss: 1.3122947664409876, CNN Loss: 0.66245829876489\n",
    "# Epoch [9/10], Step [8000/12500], ANN Loss: 1.3103235375266522, CNN Loss: 0.654794091342861\n",
    "# Epoch [9/10], Step [10000/12500], ANN Loss: 1.3175036150328816, CNN Loss: 0.6717786931779482\n",
    "# Epoch [9/10], Step [12000/12500], ANN Loss: 1.3396734294369816, CNN Loss: 0.6851124844889855\n",
    "# Epoch [10/10], Step [2000/12500], ANN Loss: 1.2198837846461683, CNN Loss: 0.5523294631745921\n",
    "# Epoch [10/10], Step [4000/12500], ANN Loss: 1.259059353346005, CNN Loss: 0.5920002504468648\n",
    "# Epoch [10/10], Step [6000/12500], ANN Loss: 1.2515505373105407, CNN Loss: 0.6000785271073276\n",
    "# Epoch [10/10], Step [8000/12500], ANN Loss: 1.3098676465936006, CNN Loss: 0.6619040474448702\n",
    "# Epoch [10/10], Step [10000/12500], ANN Loss: 1.2947476158775388, CNN Loss: 0.6574993534947862\n",
    "# Epoch [10/10], Step [12000/12500], ANN Loss: 1.3021455377489328, CNN Loss: 0.6422082813492307\n",
    "\n",
    "# >>> Beginning validation!\n",
    "# ANN validation accuracy: 42.120000000000005%, CNN validation accuracy: 68.45%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework Questions\n",
    "\n",
    "**To make sure your code produces consistent results, it is advisable to click \"Kernel -> Restart & Run All\" every time you want to run your code.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9d88fe0d5a5da473",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 1: CNN Advantage (10 points)\n",
    "\n",
    "Compute the accuracy of a simple dense neural network and a simple CNN on the dataset. Explain the results and briefly overview the advantages of a CNN over a standard neural network for image-related tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-90d19f37b669e57c",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "ANN validation accuracy is 42% whereas CNN validation accuracy is 68.45%. While training both the models started with almost similar loss on the train data set but loss started going down faster for the CNN which ultimately achieved higher accuracy.\n",
    "\n",
    "The ANN and CNN models have approximately equal number of trainable parameters.But the CNN model has convolutional layers that can extract features from the images and therefore for the image-related tasks CNN can learn the image patterns using the convolutional filters and still keeping the parameters under control by applying pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ff132f3e79a9ae46",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 2: Dropout Rate (25 points)\n",
    "\n",
    "Explain the purpose of dropout in any neural network model. In doing so, note what can happen if the dropout rate is too high and what can happen if the dropout rate is too low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1d4aaf9724eff071",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Dropout is a regularization technique used in neural network models to reduce overfitting. Overfitting occurs when a model learns the noise of the training data, leading to poor generalization on unseen data. Dropout addresses this problem by randomly dropping out (i.e., setting to zero) a proportion of the neurons in the network during training, making each neuron more independent and forces the network to learn more robust features that are not reliant on the presence of specific neurons.\n",
    "\n",
    "If the dropout rate is too high, then too many neurons are dropped out during training, and the network will not be able to learn effectively. The model's performance will suffer, and it may even fail to converge.\n",
    "\n",
    "If the dropout rate is too low, then not enough neurons are dropped out during training, and the network will continue to overfit the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e05305cff2c7415f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 3: Kernel Size (25 points)\n",
    "\n",
    "Explain the purpose of spatial filters (kernels) in a CNN. Additionally, explain where they fit into the overall architecture of the CNN in this coding example. Finally, explain what can happen if the kernel size is too large and what can happen if the kernel size is too small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5bd1497e584129ea",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Spatial filters (kernels) in a CNN are used to extract features from an image by performing a convolution operation. The purpose of these filters is to detect specific patterns or features such as edges, corners, and blobs within an image. These filters slide over the input image, element-wise multiplying the values in the filter with the corresponding values in the input image and then summing the resulting products to produce a single output value. By varying the parameters of the filters (such as size and stride), different types of features can be extracted from the image.\n",
    "\n",
    "In the given coding example, spatial filters are applied to the input image in the convolutional layers to extract features. These filters are initialized with random values and then optimized during training using backpropagation to improve the performance of the model.\n",
    "\n",
    "In a CNN architecture, spatial filters are usually placed after the input layer and before the output layer. These filters are typically applied in a series of convolutional layers, with each layer detecting increasingly complex and abstract features.\n",
    "\n",
    "If the kernel size is too large, it can lead to the loss of important details and features from the input image, resulting in reduced accuracy. On the other hand, if the kernel size is too small, it can lead to overfitting and the extraction of noisy or irrelevant features, which can also result in reduced accuracy. Therefore, it is important to choose an appropriate kernel size based on the size and complexity of the input image and the specific features being detected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-42306086a7bdf4e5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 4: Data Augmentation (40 points)\n",
    "\n",
    "Use the code snippet provided in the next box to implement data augmentation by updating the contents of box 3 and re-running the model. Compare your accuracy without and with data augmentation and explain the results. In doing so, explain the purpose of data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ef28cafc520fc2a3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(RandomRotation(degrees=[-5.0, 5.0], interpolation=nearest, expand=False, fill=0),)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms.RandomRotation(5),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ea6c603cd9e5a90a",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "After applying data augmentation using the rotation transform it appears that the train loss for CNN has gone up but the validation accuracy has slightly gone up.\n",
    "\n",
    "Previous CNN train loss:0.64\n",
    "\n",
    "Previous CNN validation accuracy:68.45%\n",
    "\n",
    "Post data augmentation CNN train loss:0.70\n",
    "\n",
    "Post data augmentation CNN validation accuracy:69.1%\n",
    "\n",
    "The purpose of data augmentation is to increase the size and diversity of the training set, which can help to prevent overfitting and improve the generalization performance of the model. By applying random transformations to the input images, we can create new training examples that are similar to the original ones but differ in small ways, such as orientation, scale, or position. This can help the model learn to be more robust and invariant to these variations in the input data.\n",
    "\n",
    "Data augmentation can lead to increased generalization accuracy however the amount of improvement may depend on the specific dataset, model architecture, and choice of data augmentation techniques. It is therefore important to experiment with different data augmentation strategies and evaluate their impact on the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
