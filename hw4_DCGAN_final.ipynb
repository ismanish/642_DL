{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e75b96c",
   "metadata": {},
   "source": [
    "# Homework 4: Deep Convolutional Generative Adversarial Network (DCGAN) (100 points)\n",
    "\n",
    "In this assignment, we're going to build and train a DCGAN model to generate synthetic pokemon images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49ea2dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89408461",
   "metadata": {},
   "source": [
    "# Loading Data (0 points)\n",
    "\n",
    "This is an example of using a built-in torchvision dataloader for a vision dataset, no code needed here.\n",
    "\n",
    "The original data can be retrieved from: [Kaggle - Pokemon Image Dataset](https://www.kaggle.com/datasets/kvpratama/pokemon-images-dataset)\n",
    "\n",
    "**Please note** that you will want to unzip the dataset (in `assets/data/pokemon.zip`) before you run this code for the first time. As the dataset is large, you may expect it to take about twenty minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375fb64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from io import BytesIO\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load the zipped file into memory\n",
    "with open('assets/data/pokemon.zip', 'rb') as f:\n",
    "    zip_bytes = BytesIO(f.read())\n",
    "\n",
    "# Open the zip file using the ZipFile module\n",
    "with zipfile.ZipFile(zip_bytes, 'r') as zip_ref:\n",
    "    # Extract the contents of the zip file to a temporary directory\n",
    "    zip_ref.extractall('assets/data/pokemon_temp')\n",
    "\n",
    "# Load the data using the ImageFolder function, pointing to the temporary directory\n",
    "# pokemon= torchvision.datasets.ImageFolder('assets/data/pokemon_temp', transform=transforms.ToTensor())\n",
    "pokemon= torchvision.datasets.ImageFolder('assets/data/pokemon_temp')\n",
    "\n",
    "# Delete the temporary directory\n",
    "# shutil.rmtree('assets/data/pokemon_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4511d533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # d2l.DATA_HUB['pokemon'] = (d2l.DATA_URL + 'pokemon.zip',\n",
    "# #                            'c065c0e2593b8b161a2d7873e42418bf6a21106c')\n",
    "\n",
    "# # data_dir = d2l.download_extract('pokemon', folder='assets/data')\n",
    "\n",
    "# pokemon = torchvision.datasets.ImageFolder('assets/data/pokmeon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f522b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "# the pipeline applied to images: resizing, scaling, normalizing\n",
    "transformer = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((64, 64)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(0.5, 0.5)])\n",
    "pokemon.transform = transformer\n",
    "\n",
    "# build data loader\n",
    "data_iter = torch.utils.data.DataLoader(\n",
    "    pokemon, batch_size=batch_size, shuffle=True,\n",
    "    num_workers=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565c6699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a batch of data\n",
    "d2l.set_figsize((4, 4))\n",
    "for X, y in data_iter:\n",
    "    imgs = X[0:20, :, :, :].permute(0, 2, 3, 1) / 2 + 0.5\n",
    "    d2l.show_images(imgs, num_rows=4, num_cols=5)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400715fb",
   "metadata": {},
   "source": [
    "# Question 1: Build the Generator\n",
    "\n",
    "In this part, you will build the generator of a DCGAN. Each generator is composed of several generator blocks.\n",
    "\n",
    "Each generator block should include a convolutional layer, batch normalization, and an activation function.\n",
    "\n",
    "The generator then consists of 4 blocks with different input/output channels, a convolutional layer and an activation function.\n",
    "\n",
    "Finally, we will put the blocks into an nn.Sequential module as we did in Homework 2.\n",
    "\n",
    "You can follow the architecture proposed in the original paper: [Unsupervised Representation Learning\n",
    "with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/pdf/1511.06434.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b398ac2b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "22312d8a94a63205c2c5e5e4801fbf42",
     "grade": false,
     "grade_id": "cell-78a202549fd19091",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class G_block(nn.Module):\n",
    "    def __init__(self, out_channels,in_channels=3, kernel_size=4, stride=2, padding=1, normalize=True):\n",
    "        super(G_block, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False))\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aa4a93",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "81447f1d6419593f9502b6b2a9615565",
     "grade": false,
     "grade_id": "cell-dfe67fe0dfe98bd5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "net_G = nn.Sequential(\n",
    "    G_block(100, 512, normalize=False),  # input: (batch_size, 100, 1, 1), output: (batch_size, 512, 4, 4)\n",
    "    G_block(512, 256),  # input: (batch_size, 512, 4, 4), output: (batch_size, 256, 8, 8)\n",
    "    G_block(256, 128),  # input: (batch_size, 256, 8, 8), output: (batch_size, 128, 16, 16)\n",
    "    G_block(128, 64),  # input: (batch_size, 128, 16, 16), output: (batch_size, 64, 32, 32)\n",
    "    nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),  # input: (batch_size, 64, 32, 32), output: (batch_size, 3, 64, 64)\n",
    "    nn.Tanh()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8a3ac8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2ac5b63ce5bac248962948225114474",
     "grade": true,
     "grade_id": "cell-e7256dccd173fb37",
     "locked": true,
     "points": 50,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# assert statements to help check your solutions \n",
    "\n",
    "gb=G_block(100)\n",
    "conv_num=0\n",
    "batch_norm=0\n",
    "for m in gb.modules():\n",
    "    conv_num+=(type(m)==nn.ConvTranspose2d)\n",
    "    batch_norm+=(type(m)==nn.BatchNorm2d)\n",
    "assert conv_num==1, \"Question 1: Number of 2D transposed convolution operators applied in G_block class does not match expected\"\n",
    "assert batch_norm==1, \"Question 1: Number of batch normalizations applied does not match expected\"\n",
    "\n",
    "\n",
    "\n",
    "gblock_num=0\n",
    "conv_num=0\n",
    "for m in net_G:\n",
    "    gblock_num+=(type(m)==G_block)\n",
    "    conv_num+=(type(m)==nn.ConvTranspose2d)\n",
    "assert gblock_num==4, \"Question 1: Number of generator blocks does not match expected\"\n",
    "assert conv_num==1, \"Question 1: Number of 2D transposed convolution operators applied in DCGAN generator does not match expected\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b1d883",
   "metadata": {},
   "source": [
    "# Question 2: Build the Discriminator\n",
    "\n",
    "In this question, you will build the generator of a DCGAN. Each generator is composed of several generator blocks.\n",
    "\n",
    "Each generator block should include a convolutional layer, batch normalization, and an activation function.\n",
    "\n",
    "The generator then consists of 4 blocks with different input/output channels, and a convolutional layer.\n",
    "\n",
    "Finally, we will put the blocks into an nn.Sequential module as we did in Homework 2.\n",
    "\n",
    "You can follow the architecture proposed in the original paper: [Unsupervised Representation Learning\n",
    "with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/pdf/1511.06434.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8098436",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4a6505cfd0d7aa16a770d2af94999de",
     "grade": false,
     "grade_id": "cell-8d351ea148c72fb2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class D_block(nn.Module):\n",
    "    def __init__(self, out_channels, in_channels=3, kernel_size=4, strides=2,\n",
    "                 padding=1, alpha=0.2, **kwargs):\n",
    "        super(D_block, self).__init__(**kwargs)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = nn.LeakyReLU(alpha)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.conv(X)\n",
    "        out = self.bn(out)\n",
    "        out = self.activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06302b5e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d4c208ad000bf132388d51a38d22b3f4",
     "grade": false,
     "grade_id": "cell-ef4f3584d1b25105",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "net_D = nn.Sequential(\n",
    "    # input size: (3, 64, 64)\n",
    "    D_block(64, in_channels=3), # output size: (64, 32, 32)\n",
    "    D_block(128),                # output size: (128, 16, 16)\n",
    "    D_block(256),                # output size: (256, 8, 8)\n",
    "    D_block(512),                # output size: (512, 4, 4)\n",
    "    nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "    nn.Sigmoid()  # Output: (1, 1, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5b59c8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b25709bd848e3494bb6119811a16752",
     "grade": true,
     "grade_id": "cell-1d9c97e64f0fc62f",
     "locked": true,
     "points": 50,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# assert statements to help check your solutions\n",
    "\n",
    "db=D_block(100)\n",
    "conv_num=0\n",
    "batch_norm=0\n",
    "for m in db.modules():\n",
    "    conv_num+=(type(m)==nn.Conv2d)\n",
    "    batch_norm+=(type(m)==nn.BatchNorm2d)\n",
    "assert conv_num==1, \"Question 2: Number of 2D transposed convolution operators applied in D_block class does not match expected\"\n",
    "assert batch_norm==1, \"Question 2: Number of batch normalizations applied does not match expected\"\n",
    "\n",
    "\n",
    "\n",
    "dblock_num=0\n",
    "conv_num=0\n",
    "for m in net_D:\n",
    "    dblock_num+=(type(m)==D_block)\n",
    "    conv_num+=(type(m)==nn.Conv2d)\n",
    "assert dblock_num==4, \"Question 2: Number of discriminator blocks does not match expected\"\n",
    "assert conv_num==1, \"Question 2: Number of 2D transposed convolution operators applied in DCGAN discriminator does not match expected\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d55466",
   "metadata": {},
   "source": [
    "# Train the Network\n",
    "\n",
    "Since GANs can be hard to train, we provide the full training code, so no code is needed here. But feel free to tune the network yourself. Please don’t remove the print statement included in the ``train`` function, as the instructional team will be using it to check whether your loss decreases monotonically or not.\n",
    "\n",
    "You should expect to find that the loss of the generator generally increases while the loss of the discriminator decreases over the epochs. The output images will potentially `real`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee39933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net_D, net_G, data_iter, num_epochs, lr, latent_dim,\n",
    "          device=d2l.try_gpu()):\n",
    "    loss = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "    for w in net_D.parameters():\n",
    "        nn.init.normal_(w, 0, 0.02)\n",
    "    for w in net_G.parameters():\n",
    "        nn.init.normal_(w, 0, 0.02)\n",
    "    net_D, net_G = net_D.to(device), net_G.to(device)\n",
    "    trainer_hp = {'lr': lr, 'betas': [0.5, 0.999]}\n",
    "    trainer_D = torch.optim.Adam(net_D.parameters(), **trainer_hp)\n",
    "    trainer_G = torch.optim.Adam(net_G.parameters(), **trainer_hp)\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[1, num_epochs], nrows=2, figsize=(5, 5),\n",
    "                            legend=['discriminator', 'generator'])\n",
    "    animator.fig.subplots_adjust(hspace=0.3)\n",
    "    for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "        # Train one epoch\n",
    "        timer = d2l.Timer()\n",
    "        metric = d2l.Accumulator(3)  # loss_D, loss_G, num_examples\n",
    "        for X, _ in data_iter:\n",
    "            batch_size = X.shape[0]\n",
    "            Z = torch.normal(0, 1, size=(batch_size, latent_dim, 1, 1))\n",
    "            X, Z = X.to(device), Z.to(device)\n",
    "            metric.add(d2l.update_D(X, Z, net_D, net_G, loss, trainer_D),\n",
    "                       d2l.update_G(Z, net_D, net_G, loss, trainer_G),\n",
    "                       batch_size)\n",
    "        # Show generated examples\n",
    "        Z = torch.normal(0, 1, size=(21, latent_dim, 1, 1), device=device)\n",
    "        # Normalize the synthetic data to N(0, 1)\n",
    "        fake_x = net_G(Z).permute(0, 2, 3, 1) / 2 + 0.5\n",
    "        imgs = torch.cat([\n",
    "            torch.cat([fake_x[i * 7 + j].cpu().detach()\n",
    "                       for j in range(7)], dim=1)\n",
    "            for i in range(len(fake_x) // 7)], dim=0)\n",
    "        animator.axes[1].cla()\n",
    "        animator.axes[1].imshow(imgs)\n",
    "        # Show the losses\n",
    "        loss_D, loss_G = metric[0] / metric[2], metric[1] / metric[2]\n",
    "        animator.add(epoch, (loss_D, loss_G))\n",
    "    print(f'loss_D {loss_D:.3f}, loss_G {loss_G:.3f}, '\n",
    "          f'{metric[2] / timer.stop():.1f} examples/sec on {str(device)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3fe59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network on GPU\n",
    "# See how the loss of D and G change over time, and how the final image generations look like.\n",
    "latent_dim, lr, num_epochs = 100, 0.005, 20\n",
    "train(net_D, net_G, data_iter, num_epochs, lr, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2259b3f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94467345",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "mads_deep_learning_v1_hw4-DCGAN"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
